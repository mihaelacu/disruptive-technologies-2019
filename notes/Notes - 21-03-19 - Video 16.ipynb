{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 16. Decision Tree Algorithm\n",
    "***\n",
    "\n",
    "- Decision tree starts with a problem\n",
    "- Decision tree is a tree shaped diagram used to determine a course of action. Each branch of the tree represents a possible decision, occurrence or reaction\n",
    "\n",
    "\n",
    "### Problems Decision Trees can Solve\n",
    "- **Classification**\n",
    "    - A classification tree will determine a set of logical if-then conditions to classify problems\n",
    "    - For example, disciminating between three types of flowers based on certain features\n",
    "- **Regression**\n",
    "    - Regression tree is used when the target variable is numerical or continious in nature\n",
    "    - We fit a regression model to the target variable using each of the independent variables\n",
    "    - Each split is made based on the sum of squared error\n",
    "    \n",
    "    \n",
    "### Advantages of Decision Tree\n",
    "- Simple to understand, interpret and visualize\n",
    "- Little effort is required for data preparation\n",
    "- Can handle both numerical and categorical data\n",
    "- Non-linear parameters don't effect its performance\n",
    "    - even if data doesn't fit nicely on a graph it can still be used to make predictions\n",
    "    \n",
    "    \n",
    "### Disadvantages of Decision Tree\n",
    "- Overfitting\n",
    "    - occurs when the algorithm captures noise in the data\n",
    "- High variance\n",
    "    - the model can get unstable due to small variation in data\n",
    "- Low biased tree\n",
    "    - a highly complicated Decision tree tends to have a low vias which makes it difficult for the model to work with new data\n",
    "    \n",
    "    \n",
    "### Important Terms\n",
    "**ENTROPY**\n",
    "- Measure of randomness or unpredictability in the dataset\n",
    "\n",
    "**INFORMATION GAIN**\n",
    "- It is the measure of decrease in entropy after the dataset is split\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 100%;text-align: center;\">\n",
    "    <img src=\"img/210319/1.png\"><br><a href=\"https://youtu.be/RmajweUFKvM?list=PLEiEAq2VkUULYYgj13YHUWmRePqiu8Ddy\"><b>Image Source</b></a></div>\n",
    "    \n",
    "**LEAF NODE**\n",
    "- Carries the classification or the decision\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 100%;text-align: center;\">\n",
    "    <img src=\"img/210319/2.png\"><br><a href=\"https://youtu.be/RmajweUFKvM?list=PLEiEAq2VkUULYYgj13YHUWmRePqiu8Ddy\"><b>Image Source</b></a></div>\n",
    "    \n",
    "**ROOT NODE**\n",
    "- The top most decision node is known as the root node\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 100%;text-align: center;\">\n",
    "    <img src=\"img/210319/3.png\"><br><a href=\"https://youtu.be/RmajweUFKvM?list=PLEiEAq2VkUULYYgj13YHUWmRePqiu8Ddy\"><b>Image Source</b></a></div>\n",
    "    \n",
    "    \n",
    "### How Does a Decision Tree Work?\n",
    "- Here are some animals:\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 100%;text-align: center;\">\n",
    "    <img src=\"img/210319/4.png\"><br><a href=\"https://youtu.be/RmajweUFKvM?list=PLEiEAq2VkUULYYgj13YHUWmRePqiu8Ddy\"><b>Image Source</b></a></div>\n",
    "    \n",
    "- *Problem Statement*: classify the different types of animals based on their features using decision tree\n",
    "- The dataset is looking quite messy and the entropy is high in this case\n",
    "- Let's look at the training dataset:\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 100%;text-align: center;\">\n",
    "    <img src=\"img/210319/5.png\"><br><a href=\"https://youtu.be/RmajweUFKvM?list=PLEiEAq2VkUULYYgj13YHUWmRePqiu8Ddy\"><b>Image Source</b></a></div>\n",
    "    \n",
    "- We have to frame the conditions that split the data in such a way that the information gain is the highest\n",
    "    - Gain is the measure of decrease in entropy after splitting\n",
    "- And here's a formula for entropy:\n",
    "\n",
    "$$ \\large \\sum_{i=1}^{k} P(value_i) \\log_2 (P(value_i)) $$\n",
    "\n",
    "- Let's try to calculate the entropy for the current dataset\n",
    "- We have:\n",
    "    - 3 giraffes\n",
    "    - 2 tigers\n",
    "    - 1 monkey\n",
    "    - 2 elephants\n",
    "        - total of 8 animals\n",
    "- If we plug that in to the formula:\n",
    "\n",
    "$$ ENTROPY = (\\frac{3}{8}) \\log_2 (\\frac{3}{8}) + (\\frac{2}{8}) \\log_2 (\\frac{2}{8}) + (\\frac{1}{8}) \\log_2 (\\frac{1}{8}) + (\\frac{2}{8}) \\log_2 (\\frac{2}{8}) $$<br>\n",
    "$$ ENTROPY = 0.571 $$\n",
    "\n",
    "- We will calculate the entropy of the dataset similarly after every split to calculate the gain\n",
    "- Gain can be calculated by finding the difference of the subsequent entropy values after split\n",
    "- Now we will try to choose a condition that gives us the highest gain\n",
    "- We will do that by splitting the data using each condition and checking the gain that we get out them\n",
    "- Here are possible conditions:\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 100%;text-align: center;\">\n",
    "    <img src=\"img/210319/6.png\"><br><a href=\"https://youtu.be/RmajweUFKvM?list=PLEiEAq2VkUULYYgj13YHUWmRePqiu8Ddy\"><b>Image Source</b></a></div>\n",
    "    \n",
    "- We will split the data by the color yelow:\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 100%;text-align: center;\">\n",
    "    <img src=\"img/210319/7.png\"><br><a href=\"https://youtu.be/RmajweUFKvM?list=PLEiEAq2VkUULYYgj13YHUWmRePqiu8Ddy\"><b>Image Source</b></a></div>\n",
    "    \n",
    "- The entropy after splitting has decreased considerably\n",
    "- However, we still need some splitting at both the branches to attain an entropy value equal to zero\n",
    "- So, we decide to split both the nodes using **height** as condition:\n",
    "\n",
    "<div style=\"display: block;margin-left: auto;margin-right: auto;width: 100%;text-align: center;\">\n",
    "    <img src=\"img/210319/8.png\"><br><a href=\"https://youtu.be/RmajweUFKvM?list=PLEiEAq2VkUULYYgj13YHUWmRePqiu8Ddy\"><b>Image Source</b></a></div>\n",
    "    \n",
    "- Since every branch now contains single label type, we can say that the entropy in this case has reached the least value\n",
    "- This tree can now predict all the classes of animals present in the dataset with 100% accuracy\n",
    "\n",
    "\n",
    "### Use Case - Loan Repayment Prediction\n",
    "- Individual says: *I need to find out if my customers are going to return the loan they took from my bank or not*\n",
    "- Predict if a customer will repay loan amount or not using decision tree algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import the libraries\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn import tree\n",
    "        \n",
    "Link to the dataset\n",
    "\n",
    "        data = pd.read_csv(\"data\")\n",
    "        \n",
    "Separating the target variables\n",
    "\n",
    "        X = data.values[:, 1:5]\n",
    "        Y = data.values[:, 0]\n",
    "        \n",
    "Splitting dataset into training and test\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        \n",
    "Function to perform training with Entropy\n",
    "\n",
    "        clf_entropy = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, min_samples_leaf=5)\n",
    "        clf_entropy.fit(X_train, y_train)\n",
    "        \n",
    "Function to make Prediction\n",
    "\n",
    "        y_pred = clf_entropy.predict(X_test)\n",
    "        \n",
    "Checking accuracy\n",
    "\n",
    "        print(\"Accuracy is: {}\".format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
